\documentclass[a4paper, UKenglish, cleveref, autoref, thm-restate]{lipics-v2021}

\bibliographystyle{plainurl}

\title{Fast Random Integer Generation in an Interval} 

\author{Lukas Geis}{Goethe UniversitÃ¤t Frankfurt am Main}{s2949316@stud.uni-frankfurt.de}{}{}

\authorrunning{L. Geis} 

\Copyright{Lukas Geis}

\ccsdesc[100]{Mathematics of computing $\rightarrow$ Random number generation}

\keywords{Random Number Generation, Rejection Method, Randomized Algorithms, Algorithm Engineering} 

\category{} 
\relatedversion{} 
\supplement{Sources of implementations and experiments can be accessed at \url{https://github.com/lukasgeis/FRIGiaI-Review}}
\acknowledgements{}
\nolinenumbers 


%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Packages
\usepackage{mathtools}
\usepackage{nicefrac}
\newcommand{\Mod}[1]{\ \mathrm{mod}\ #1}

\usepackage[vlined,ruled,linesnumbered]{algorithm2e}
\DontPrintSemicolon
\SetKwInOut{Require}{Require}

\usepackage{hyperref}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}} % Centered Multilines
\renewcommand{\arraystretch}{1.5}
\usepackage{booktabs} % Rules
\usepackage{pifont} % checkmark / cross - thanks to `https://tex.stackexchange.com/questions/42619/xmark-that-complements-the-ams-checkmark`
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}


\begin{document}

\maketitle


%%%%% Abstract
\begin{abstract}
    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent convallis orci arcu, eu mollis dolor. Aliquam eleifend suscipit lacinia. Maecenas quam mi, porta ut lacinia sed, convallis ac dui. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse potenti. 
\end{abstract}


%%%%% Introduction
\section{Introduction}
\subsection{Motivation}
The study of randomized algorithms goes as far back as the 1910s~\cite{MathComp}.
While initially small, this study is now scientifically pursued in almost all parts of computer science.
Not only in theory, but also in practice, randomized algorithms gain even more importance today to not only solve specific algorithmic problems but also to create large, scalable, and unbiased data sets that can be used in a variety of fields.
\\
The core of such algorithms is most often the pseudo-random number generator (PRNG).
There are many efficient PRNGs such as XorShift~\cite{XS}, MersenneTwister~\cite{MT}, linear congruential generators~\cite{LinConGen, MINSTD, LM}, and younger generators such as the PCG-Family~\cite{PCG} and ChaCha~\cite{Cha}.
These generators produce $32$-bit or $64$-bit words which we can interpret as unsigned integers in $[0,2^{32})$ and $[0,2^{64})$ respectively.
However, for most applications we do not want to have such a large range of possible random values but instead want to confine the range to a given interval $[a,b]$.
This can not only be used to uniformly sample an element from an array but also for more complex algorithms
\begin{itemize}
    \item The Fisher-Yates random shuffle~\cite{FY} (see \hyperref[alg:fy]{Algorithm 1}) uniformly permutes an array of $n$ elements in time $\Theta(n)$. 
    We have to draw a random integer in an interval for every shuffled element.
    \item A random graph model describes a distribution over a (parametrized) family of graphs. 
    There exist an uncountable number of such models and almost all of them require some sort of integer sampling in an interval. 
    For example, the $\mathcal{G}(n,m)$ model~\cite{GNM} uniformly samples a (directed) graph with $n$ nodes and $m$ edges. 
    A possible algorithm for this problem is Reservoir-Sampling~\cite{Reservoir}(see \hyperref[alg:gnm]{Algorithm 2}) which is a general technique that can sample $k$ elements without replacement from an input stream. 
    Another model is the BA model~\cite{BABook,BA} which iteratively builds the graph by adding nodes with edges to previously added nodes. %(see \hyperref[alg:ba]{Algorithm 3}). 
    Both models require sampling of an integer in an interval for each of its edges (or more).
    \item An Alias-Table~\cite{Alias} can be used to simulate static discrete distributions in practice. 
    After creating and initializing the table, we can sample in (expected) time $\Theta(1)$ by drawing a uniform random row and offset in each round. 
    Hence, we need to sample an integer uniformly at random in $[0,n)$ where $n$ is the number of rows in the table.
\end{itemize}


%%% BEGIN Fisher-Yates-Shuffle
\begin{algorithm}[!htb] \label{alg:fy}
    \caption{Fisher-Yates-Shuffle: uniformly shuffle an array of size $n$}
    \Require{source of uniformly-distributed random integers in $[0,i]$ for parameter $i$}
    \Require{array $A$ of size $n$}
    \For{$i = n - 1,\ldots,1$}{
        $j \leftarrow$ random integer in $[0,i]$\;
        swap $A[i]$ and $A[j]$\;
    }
\end{algorithm}
%%% END Fisher-Yates-Shuffle


%%% BEGIN G(n,m) Sampling
\begin{algorithm}[!htb] \label{alg:gnm}
    \caption{Sampling of (directed) $\mathcal{G}(n,m)$ graphs using Reservoir Sampling}
    \Require{source of uniformly-distributed random integers in $[0,i]$ for parameter $i$}
    \Require{number of nodes $n$ and number of edges $m \leq n^2$}
    $E \leftarrow$ array (of edges) of size $m$\;
    \For{$i = 0,\ldots,m - 1$}{
        $E[i] \leftarrow \left(i \div n, i \mod n\right)$\;
    }
    \For{$i = m,\ldots,n^2$}{
        $j \leftarrow$ random integer in $[0,i]$\;
        \If{$j < m$}{
            $E[j] \leftarrow \left(i \div n, i \mod n\right)$\;
        }
    }
    \KwRet{$E$}\;
\end{algorithm}
%%% END G(n,m) Sampling


%%% This block will not be included in the final version but is currently here in case the feedback says, it is useful and not over the top
\iffalse
%%% BEGIN BA-Sampling
\begin{algorithm}[!htb] \label{alg:ba}
    \caption{Preferential-Attachment-Generator \cite{BASampling}: sample BA-graphs in linear time}
    \Require{source of uniformly-distributed random integers in $[0,i]$ for parameter $i$}
    \Require{inital edge-list $E_0$ with $n_0$ nodes and $m_0 \coloneqq |E_0|$ edges, number of additional nodes $N$, neighbor-parameter $\nu$}
    $M \leftarrow$ array of size $2\left(m_0 + \nu N\right)$\;
    \For{$i = 0,\ldots,m_0 - 1$}{
        $\left(M[2i], M[2i + 1]\right) \leftarrow E_0[i]$\;
    }
    \For{$i = 0,\ldots,N - 1$}{
        \For{$j = 1,\ldots,\nu$}{
            $k \leftarrow$ random integer in $[0,2(m_0 + i\nu) - 1]$\;
            $M[2(m_0 + i\nu)] \leftarrow M[k]$\;
            $M[2(m_0 + i\nu) + 1] \leftarrow n_0 + i$\;
        }
    }
    $E \leftarrow$ array (of edges) of size $\left(m_0 + \nu N\right)$\;
    \For{$i = 0,\ldots,m_0 + \nu N - 1$}{
        $E[i] \leftarrow (M[2i], M[2i + 1])$\;
    }
    \KwRet{$E$}\;
\end{algorithm}
%%% END BA-Sampling
\fi

\subsection{Preliminaries}\label{sec:1.2}
We denote by $x \div y$ the integer division operation, namely $\lfloor\nicefrac{x}{y}\rfloor$, and the remainder operation by $x \Mod y \equiv x - (x \div y)y$. 
Divisions by a power of two can be efficiently implemented by Bit-\textsc{RightShift} denoted by $\gg$: $x \div 2^W \equiv x \gg W$.
Similarly, we denote a Bit-\textsc{LeftShift} by $\ll$.
Additionally, we can efficiently compute the remainder of a division py a power of two by a logical \textsc{And}-operation, denoted by $\&$: $x \Mod 2^W \equiv x \And (2^W - 1)$.
Typically, PRNGs generate uniform random numbers in an interval $[0,2^W)$ where $W \in \{32,64\}$, namely a uniform random $W$-bit number.

Since we can map every integer in an interval $[a,b)$ to the interval $[0,b - a)$ by subtracting $a$, we want to generate a uniform random integer in an interval $[0,n)$ for a given $n$.
If we map all integers $x$ in $[0,2^W)$ with the function $x \Mod n$, we get the following sequence:
{\large
\begin{align}\label{eq:1}
    \overbrace{\underbrace{\overbrace{0,1,\ldots,n - 1}^{\text{$n$ values}},\overbrace{0,1,\ldots,n - 1}^{\text{$n$ values}},\ldots,\overbrace{0,1,\ldots,n - 1}^{\text{$n$ values}}}_{\text{$\left(2^W \div n\right) \cdot n$ values}},\underbrace{0,1,\ldots,\left(2^W \Mod n\right) - 1}_{\text{$2^W \Mod n$ values}}}^{\text{$2^W$ values}}
\end{align}
}%
This motivates the following lemma:
\begin{lemma}[Remainder Multiples] \label{lemma:1}
    For given integers $a,b,n > 0$ with $a < b$, there exist exactly $(b - a) \div n$ integers $x \in [a,b)$ for every $0 \leq r < n$ such that $x \Mod n \equiv r$ whenever $n$ divides $b - a$.
\end{lemma}




%%%%% Techniques
\section{Existing Techniques}\label{sec:2}
In this section, we will review a number of different techniques used for sampling a uniform random integer in a given range $[0,n)$.
Throughout this section, we use {\texttt rand()} to indicate the drawing of a uniform random $W$-bit number.

\subsection{Biased}
While our goal is to generate \emph{uniform} random numbers, there are also some techniques that generate biased - thus not uniform - random integers in a given range in exchange for much less complexity.

\subsubsection{Modulo Reduction}\label{sec:2.1.1}
The most common and under-egineered algorithm is the modulo reduction, namely
\begin{center}
    $\texttt{rand()} \Mod n$
\end{center}
Clearly, this method returns a random integer in the interval $[0,n)$.
However, as seen in \hyperref[eq:1]{equation (1)}, for $n$ that do not perfectly divide $2^W$, we have a \emph{leftover} interval at the end that is biased.
Thus, not all integers in $[0,n)$ are equally likely and this reduction does not produce uniform numbers.

Furthermore, while this approach is probably the most intuitive and simplistic, it also illustrates why drawing numbers within a range is typically slow.
We require a remainder operation which can be implemented through the division (\texttt{div}) instruction.
For $32$-bit registers, this instruction has a latency of $26$ cycles and for $64$-bit registers, the latency lies within the range of $35$ to $88$ cycles~\cite{Instructions}.


\subsubsection{Floating-Point Conversion}\label{sec:2.1.2}
A common method to not rely on the costly \texttt{div} instruction is using a fixed-point floating-point conversion which consists of the following steps:
\begin{enumerate}
    \item Draw $x = \texttt{rand()}$ and convert $x$ to a floating-point number.
    \item Multiply $x$ with $2^{-W}$ to generate a floating point number $y \in [0,1)$.
    \item Multiply $y$ with $n$ and round it down to an integer before returning the result.
\end{enumerate}

Again, this method clearly generates a random integer in $[0,n)$ because $y$ is less than $1$ and we multiply with $n$ before rounding down.
However, in the typical floating-point standard (IEEE 754) with $32$ bits, we have a mantisse of $24$ bits and hence can at best represent all values in $[0,2^{24})$ divided by $2^{24}$.
Thus, we do not get the same accuracy as we would have with a $32$-bit number and hence can not generate all numbers in $[0,n)$ if $n > 2^{24}$.
A solution would be to instead use double precision floating-point numbers that have a mantisse of $53$ bits and thus can represent all values in $[0,2^{53})$ divided by $2^{53}$.
But this again is not enough if $W = 64$ where we require a mantisse of at least $64$ bits which is not accessible on all systems (long double).
Furthermore, the conversion also comes at a price: the corresponding instructions have a latency of at least $6$ cycles on Skylake processors~\cite{Instructions}.


\subsubsection{Multiply-and-Shift}\label{sec:2.1.3}
We can adjust the prior floating-point conversion method to only use fixed-point arithmetic rather than floating-point arithmetic:
\begin{center}
    $\left(\texttt{rand()} \cdot n\right) \gg W$
\end{center}
where $\gg W$ is equivalent to dividing by $2^W$ as mentioned in \hyperref[sec:1.2]{section 1}.
Clearly, this method and the floating-point method before work the same way, hence creating the same bias - but here, we multiply first before dividing by $2^W$~\cite{MultShift}.
On the other hand, by multiplying with $n$ first, we get a number in $[0,n \cdot 2^W)$ instead.
If we now divide by $2^W$, we map all multiples of $n$ in $[0,2^W)$ to $0$, all multiples of $n$ in $[2^W, 2 \cdot 2^W)$ to $1$, and so on such that we map all multiples of $n$ in $[i \cdot 2^W, (i + 1) \cdot 2^W)$ (the $(i + 1)$\textsuperscript{th} $2^W$-interval) to $i$.

Since we might get a number much bigger than $2^W$ and thus need a new representation with more than $W$ bits.
For $W = 32$, we simply switch to a $64$-bit number, but for $W = 64$, we have to switch to a $128$-bit number which is inherently slower than $32$-bit or $64$-bit numbers.
Alternatively, for $W = 32$, a good compiler could compile this into a $32$-bit \texttt{mult} instruction which generates a $32$-bit number for the first $32$ bits and a $32$-bit number for the last $32$ bits.


\subsection{Unbiased}
While all prior approaches are quite simplistic and always require only a handful of instructions, they also generate a bias which can vary depending on $n$.
But since we want to generate a \emph{uniform} random number in $[0,n)$, we somehow need to get rid of such bias by incorporating a fail-safe: rejection sampling.

\subsubsection{OpenBSD}\label{sec:2.2.1}
We somehow want to get rid of the bias generated by the last interval in \hyperref[eq:1]{equation (1)}.
The key idea is that this interval does not have to be on the right.
Instead of rejecting all values generated by a random $x \geq 2^W - (2^W \Mod n)$, we could also reject all values generated by a random $x < 2^W \Mod n$ for uniformity since the interval $[2^W \Mod n, 2^W)$ has size $2^W - (2^W \Mod n)$ which is divisible by $n$.
Due to \hyperref[lemma:1]{Lemma 1}, for every $r \in [0,n)$, we have exactly $(b - a) \div n$ numbers in $[2^W \Mod n,2^W)$ with remainder $r$, thus having a uniform distribution over $[0,n)$ when computing the remainder.
This yields \hyperref[alg:openbsd]{Algorithm 3} which stems from the C standard library in OpenBSD and macOS (\texttt{arc4random\_uniform}), and has been adopted by the Go language (\texttt{Int63n, Int31n}) with slight implementation differences~\cite{GoLang} as well as the GNU C++ standard library~\cite{GnuCpp}.

%%% BEGIN OpenBSD
\begin{algorithm}[!htb] \label{alg:openbsd}
    \caption{The OpenBSD algorithm.}
    \Require{source of uniformly-distributed random integers in $[0,2^W)$ given by \texttt{rand()}}
    \Require{size of target interval given by $n$}
    $t \leftarrow \left(2^W - n\right) \Mod n$\tcc*[r]{$ \left(2^W - n\right) \Mod n \equiv 2^W \Mod n$}
    $x \leftarrow$ \texttt{rand()}\;
    \While{$x < t$}{
        $x \leftarrow$ \texttt{rand()}\;
    }
    \KwRet{$x \Mod n$}\;
\end{algorithm}
%%% END OpenBSD

In comparison to the biased techniques, we now may have to sample more than one random integer in $[0,2^W)$ to generate an integer in $[0,n)$.
Note that the number of random $W$-bit numbers drawn follows a geometric distribution with a success probability of $p = 1 - \frac{2^W \Mod n}{2^W}$.
Thus on average, we need $\frac{1}{p}$ random $W$-bit numbers which is less than $2$ because $p > \frac{1}{2}$, irrespective of $n$.
We also always require two remainder operations: one to compute the threshold that determines whether we reject a drawn random number and one to use the modulo reduction to generate the final number.


\subsubsection{Java}\label{sec:2.2.2}
Since we expect remainder operations to have high latency, always needing to compute two remainders can be quite costly.
Hence, the Java language, in its \texttt{Random class}, uses a different approach.
The key insight is that when we draw a random integer $x \in [0,2^W)$ and compute its remainder $r = x \Mod n$, we can map all integers in $[0,2^W)$ to a multiple of $n$ by computing $x - r$.
Then, every number in the last interval in \hyperref[eq:1]{equation (1)} gets mapped to the last multiple $y$ of $n$ which is not greater than $2^W$.
Note that this implies $y > 2^W - n$.
Every other integer is mapped to a different (and thus smaller) multiple $z$ of $n$ for which $z \leq y - n$ and thus $z \leq 2^W - n$.
Consequently, if we reject $x$ if $x - r > 2^W - n$ and otherwise return $r$, we get a unbiased generator.
The details can be seen in \hyperref[alg:java]{Algorithm 4}.

%%% BEGIN Java
\begin{algorithm}[!htb] \label{alg:java}
    \caption{The Java algorithm.}
    \Require{source of uniformly-distributed random integers in $[0,2^W)$ given by \texttt{rand()}}
    \Require{size of target interval given by $n$}
    $x \leftarrow$ \texttt{rand()}\;
    $r \leftarrow x \Mod n$\;
    \While{$x - r > 2^W - n$}{
        $x \leftarrow$ \texttt{rand()}\;
        $r \leftarrow x \Mod n$\;
    }
    \KwRet{$r$}\;
\end{algorithm}
%%% END Java

Again, the number of random draws of $W$-bit numbers follows a geometric distrbution with $p = 1 - \frac{2^W \Mod n}{2^W}$ and we expect to draw $\frac{1}{p} < 2$ random $W$-bit numbers before returning.
However, this time, for every drawn random number, we also need to compute a remainder, hence we also expect $\frac{1}{p} < 2$ remainder operations.
For small $n$, this is great because with high probability, we only need to compute one remainder operation in expectation as opposed to two.

\subsubsection{Fast-Dice-Roller}\label{sec:2.2.3}
For all prior unbiased algorithms, we need to use at least one remainder operation to debias the algorithm.
JÃ©rÃ©mie Lumbroso however devised an algorithm for sampling uniform random integers in $[0,n)$ for a given $n$ by iteratively building up the final number bit-by-bit~\cite{Flips}.
In comparison to previous algorithms, we do not use the whole $W$ bits generated by the PRNG but instead only use one bit at a time building up the final number consisting of $\lceil\log_2n\rceil + 1$ bits.
If we, at some point, overshoot and generate a number bigger than $n$ (but smaller than $2n$) we take a step back by subtracting $n$ and rebuilding the necessary bits.
The algorithmic details can be seen in \hyperref[alg:flips]{Algorithm 5}.

%%% BEGIN Flips
\begin{algorithm}[!htb] \label{alg:flips}
    \caption{The Fast-Dice-Roller algorithm.}
    \Require{source of a uniformly-distributed random bit given by \texttt{flip()}}
    \Require{size of target interval given by $n$}
    $b \leftarrow 1$\;
    $x \leftarrow 0$\;
    \While{\emph{true}}{
        $b \leftarrow 2 \cdot b$\;
        $x \leftarrow 2 \cdot x + \texttt{flip()}$\;
        \If{$b \geq n$}{
            \uIf{$x < n$}{
                \KwRet{$x$}\;
            } \Else{
                $b \leftarrow b - n$\;
                $x \leftarrow x - n$\;
            }
        }
    }
\end{algorithm}
%%% END Flips

\begin{lemma}[Correctness of Fast-Dice-Roller~\cite{Flips}]
    The Fast-Dice-Roller algorithms returns a uniformly distributed random integer in $[0,n)$ and terminates in expected time $\Theta(\log n)$.
\end{lemma}
\begin{proof}
    Consider the following loop invariant: $x$ is uniformly distributed over $[0,b)$.
    At the beginning, this is trivially true as $b$ is $1$ the only value $x$ can take (and takes) is $0$.
    In line $4$ we double $b$, but also double $x$ in line $5$ before adding a parity bit which ensures uniformity in the enlarged new range.
    Since we reach line $10$ and $11$ only when $x \geq n$, $x$ must be uniformly distributed in $[n,b)$ at this point.
    Since we subtract $n$ from both $b$ and $x$, we simply shift the range from $[n,b)$ to $[0,b - n)$ maintaining uniformity.
    As $x$ is always uniformly distributed in $[0,b)$ due to this loop invariant, when we return $x$, it must also be uniformly distributed over $[0,n)$.

    For the proof of termination time, consider a less efficient variant of this algorithms that throws away all prior sampled bits when rejecting in line $9$-$11$.
    Then, in each iteration, we have $\Theta(\log n)$ steps before the condition in line $6$ is \emph{true}.
    Since we double $b$ in each iteration, we know that $b < 2n$ and hence, due to the previous loop invariant, we accept with probability $p = \nicefrac{n}{2^{\lfloor\log_2n\rfloor + 1}} > \nicefrac{1}{2}$, thus proving the lemma.
\end{proof}
Clearly, while this algorithm completely avoids any remainder or division operation, for most $n$, it is too slow.
In comparison, every prior algorithm needs constant time or expected constant time to generate a uniform random number in $[0,n)$.
Fast-Dice-Roller, on the other hand, needs expected logarithmic time for sampling which is catastrophical for bigger $n$ - a big price to pay for avoiding division (and multiplication if line $4$ and $5$ are implemented using a \textsc{BitShift}).


\subsubsection{Bitmask}\label{sec:2.2.4}
The main reason, Fast-Dice-Roller is highly impractical, is due to the fact that we build up our random number bit-by-bit instead of drawing all required bits at once.
This motivates the following algorithm (see \hyperref[alg:bitmask]{Algorithm 6}) that was adopted by Apple in the macOS Sierra release when they made their own revision to the code of \texttt{arc4random\_uniform}~\cite{Apple}.

%%% BEGIN Bitmask
\begin{algorithm}[!htb] \label{alg:bitmask}
    \caption{The Bitmask algorithm.}
    \Require{source of uniformly-distributed random integers in $[0,2^W)$ given by \texttt{rand()}}
    \Require{size of target interval given by $n$}
    $z \leftarrow$ number of leading zeros in the binary representation of $n$\;
    $m \leftarrow 1 \ll (W - z)$\tcc*[r]{$1 \ll (W - z) \equiv 2^{W - z}$}
    $m \leftarrow m - 1$\;
    $x \leftarrow \texttt{rand()} \And m$\;
    \While{$x \geq n$}{
        $x \leftarrow \texttt{rand()} \And m$\;
    }
    \KwRet{$x$}\;
\end{algorithm}
%%% END Bitmask

The key idea is to first compute $k$ such that $k$ is the smallest value with $2^k \geq n$.
We then can use a Bitmask $m = 2^k - 1$ to only take the necessary $k$ bits from our generated $W$-bit number and use rejection sampling to generate a uniform random number in $[0,n)$.
This works in expected constant time since we accept with probability $p = \nicefrac{n}{2^k} > \nicefrac{1}{2}$.

Bitmask completely avoids any type of division operation.
Instead, we have to compute the number of leading zeros in line $1$.
For C and C++, the GCC-Compiler provides the \texttt{\_\_builtin\_clz()} method to compute just that~\cite{GccCLZ}.
In Rust, we can use the \texttt{leading\_zeros()} method of the standard library~\cite{RustCLZ}.
These methods however can perform vastly different depending on the underlying architecture.
For processors which have an inbuilt \texttt{clz} instruction such as ARM~\cite{ArmCLZ}, the compiler could translate this line into such an assembly instruction.
In systems without direct support, one might to rely on more algorithmic implementations such as~\cite{ImplCLZ}.


%%%%% Lemire
\section{Nearly Divisionless Unbiased Sampling}\label{sec:3}
While completely avoiding division seems possible with the Fast-Dice-Roller and the Bitmask algorithm, it also comes at a cost: logarithmic time or a \texttt{clz} operation that might be even more expensive than a division on some systems.
Hence, Lemire proposed an unbiased algorithm that uses the approach of the Multiply-and-Shift algorithm coupled with rejection sampling to remove bias.
The algorithm (see \hyperref[alg:lemire]{Algorithm 7}) uses one divison operation at most and may also use no division operation at all.
It is the main result of the underlying paper of this report~\cite{Lemire}. 

%%% BEGIN Lemire
\begin{algorithm}[!htb] \label{alg:lemire}
    \caption{The Lemire algorithm.}
    \Require{source of uniformly-distributed random integers in $[0,2^W)$ given by \texttt{rand()}}
    \Require{size of target interval given by $n$}
    $x \leftarrow \texttt{rand()}$\;
    $m \leftarrow x \cdot n$\;
    $l \leftarrow m \Mod 2^W$\;
    \If{$l < n$}{
        $t \leftarrow \left(2^W - n\right) \Mod n$\tcc*[r]{$\left(2^W - n\right) \Mod n \equiv 2^W \Mod n$}
        \While{$l < t$}{
            $x \leftarrow \texttt{rand()}$\;
        $m \leftarrow x \cdot n$\;
        $l \leftarrow m \Mod 2^W$\;
        }
    }
    \KwRet{$m \div 2^W$}\;
\end{algorithm}
%%% END Lemire

\noindent Note that the remainder operation in line $3$ and $9$ can be computed by a logical \textsc{And} operation and the division operation in line $10$ by a Right-\textsc{BitShift} as mentioned in \hyperref[sec:1.2]{Section 1}
Thus, the only \emph{real} division operation is the remainder operation in line $5$.

Since we use the same mapping as in Multiply-and-Shift, we remind the reader of the $(i + 1)$\textsuperscript{th} $2^W$-interval introduced earlier: $[i \cdot 2^W, (i + 1) \cdot 2^W)$.
By \hyperref[lemma:1]{Lemma 1}, there exist exactly $\lfloor\nicefrac{2^W}{n}\rfloor$ multiples of $n$ in the restricted $(i + 1)$\textsuperscript{th} $2^W$-interval $[i \cdot 2^W + (2^W \Mod n), (i + 1) \cdot 2^W)$ as $n$ perfectly divides $(i + 1) \cdot 2^W - (i \cdot 2^W + (2^W \Mod n)) = 2^W - (2^W \Mod n)$.
Thus, we can remove bias if we reject all multiples of $n$ in the interval $[i \cdot 2^W, i \cdot 2^W + (2^W \Mod n))$ for every $i$.

The above algorithm does just that in lines $5$ and $6$.
But it also makes use of the following observation: since $t = 2^W \Mod n$, we have $t < n$ and thus if we have $l \geq n$, we also know that $l \geq t$.
In this case we know that we do not have to reject the current value without computing $t$ in the first place.
Thus, we can completely skip the division operation with probability $p = \frac{2^W - n}{2^W} = 1 - \frac{n}{2^W}$.



%%%%% Conclusion
\section{Conclusion}\label{sec:4}
We have seen several different algorithms that can be used to sample a random integer in an interval $[0,n)$.
The results are summarized in \hyperref[tab:1]{Table 1}.
As we introduce rejection sampling for unbiased algorithms, runtimes for those are taken as expected constant or logarithmic time, while runtimes for biased algorithms are constant overall.

%%% BEGIN Results
\begin{table}[!htb]
    \centering
    \begin{tabular}{c|P{25mm}P{25mm}P{15mm}P{15mm}}
        \toprule
        & expected number of integer division operations & maximal number of integer division operations & Unbiased? & (Expected) Runtime \\
        \midrule
        \hyperref[sec:2.1.1]{Modulo Reduction} & $1$ & $1$ & \xmark & $\Theta(1)$ \\
        \hyperref[sec:2.1.2]{Floating-Point Conversion} & $0$ & $0$ & \xmark & $\Theta(1)$ \\
        \hyperref[sec:2.1.3]{Multiply-and-Shift} & $0$ & $0$ & \xmark & $\Theta(1)$ \\
        \hyperref[sec:2.2.1]{OpenBSD} & $2$ & $2$ & \cmark & $\Theta(1)$ \\
        \hyperref[sec:2.2.2]{Java} & $\frac{2^W}{2^W - (2^W \Mod n)}$ & $\infty$ & \cmark & $\Theta(1)$ \\
        \hyperref[sec:2.2.3]{Fast-Dice-Roller} & $0$ & $0$ & \cmark & $\Theta(\log n)$ \\
        \hyperref[sec:2.2.4]{Bitmask} & $0$ & $0$ & \cmark & $\Theta(1)$ \\
        \hyperref[sec:3]{Lemire} & $\frac{n}{2^W}$ & $1$ & \cmark & $\Theta(1)$ \\
        \bottomrule
    \end{tabular}
    \caption{
        Overview over different sampling algorithms: expected/maximal number of division operations, unbiased or biased, (expected) runtime
    }
    \label{tab:1}
\end{table}
%%% END Results

While the bias is usually insignificant and negligible in most use-cases, if we sample millions of times, it can easily become apparent and strongly distort the final dataset depending on the algorithm.
Hence, it is important to generate unbiased random integers at the cost of a little execution time.
Common software libraries seem to rely on algorithms that always require at least one division, if not two or more which is inefficient on most processors.
While Bitmask has in theory zero division operations, the required \texttt{clz} operation can be as inefficient as the \texttt{div} instruction.
Hence, the algorithm introduced by Lemire is a perfect fit for modern x64 processors as for small $n$, we require no division (or equally bad) operation at all and also bound the maximum number of divisions to at most one.

For different technologies however, support for the computation of the full multiplication might be lacking such as in Graphics Processing Units (GPUs)~\cite{GPU} or DRAM Processing Units (DPUs) used for In-Memory-Processing in the \textsc{Upmem} architecture~\cite{Upmem, UpmemSDK}.
In the \textsc{Upmem-Pim} toolchain, for example, an \texttt{clz} operation seems to greatly outshine a \texttt{div} operation~\cite{UpmemRng}.





\bibliography{refs}

\end{document}
